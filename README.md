# Apache Airflow-
Apache Airflow  is a platform to programmatically author, schedule, and monitor workflows.

When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.

Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.
# Airflow-project-1
@The aim of this airflow project is to apply some transformation rules in two different datasets and 
join them as one in a single output file using saprk and automating this process using Apache Airflow.
@This repository contains datasets,psyaprk file and Python file for Airflow code.
@In this project we are reading our input data from google cloud storage and writing
it to the output path of the same bucket of google cloud storage after transformation and joining.
